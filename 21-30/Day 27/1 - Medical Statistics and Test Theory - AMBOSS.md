# Medical Statistics and Test Theory
## Summary

The evidence-based medicine aims to implement the best treatment based on objective criteria and current knowledge. The foundation for this is data collected in studies. Medical statistics now deals with how to evaluate, represent, and draw the correct conclusions from this data. This chapter explains how to formalize the various sizes of an experiment in the form of variables, how to characterize the distribution of data through measures of central tendency and dispersion, and with which parameters the results are ultimately described.

A related field is test theory: Here, the interpretation of the results of a diagnostic test is examined. To this end, the important terms sensitivity, specificity, and predictive value are introduced, which are of extraordinary relevance not only for research but also for the interpretation of results in the clinic.
## Basic Concepts and Descriptive Statistics

While physical quantities can be directly measured through an appropriate experimental setup, theoretical constructs such as intelligence cannot be immediately quantified. To capture such characteristics, empirical observable variables, known as indicators, are used. This translation of a subject of investigation into measurable variables is called operationalization and is often the most important and challenging part of a study.

The descriptive (descriptive) statistics include the statistical methods for characterizing the sample using statistical measures (e.g. mean and standard deviation) and presentation through appropriate charts and tables. In contrast, inductive (inferential) statistics examines whether a sample can be generalized to the population. It primarily utilizes probability theory. Overall, statistical analyses can be univariate, bivariate, or multivariate, depending on whether one variable is analyzed alone or the relationship between two or more variables is examined.

### Observing and Measuring Constructs

- Latent construct (= latent characteristic): A theoretical concept that is not directly observable and measurable, such as health, intelligence, quality of life, or depression, which can only be measured through indicators.
- Operationalization: Refers to the procedure for measurement as well as the measurement instruments used to assess a theoretical construct.

### Variables

In the context of studies, variables are observed and measured. The framework is formed by the independent variable, based on which the dependent variable is to be explained; therefore, the independent variable can also be referred to as the explanatory variable and the dependent variable as the explained variable. Furthermore, third variables are distinguished, which can influence the outcome of the study.

- Independent variable (= explanatory variable): A factor that is deliberately varied by the experimenter in an experiment to observe the effects on the dependent variable.
- Dependent variable (= explained variable): Variable whose change is observed as a result of the variation of the independent variables
- Confounding variable (= confounder): A variable not accounted for in the study that can influence the outcome.
- Mediator variable: Acts as a link between independent and dependent variable
- Moderator variable: Influences the effect of the independent variable and thus the outcome.
- Risk indicator: Variables that indicate an illness. They are determined descriptively in epidemiology and, unlike risk factors, do not show a causal relationship.
- Interaction effect: Effect on the dependent variable that can be generated by the interaction of two or more independent variables

### Scaling

Basically, all quantities can be summarized in variables. However, for calculations and statistical evaluation, it makes a difference whether the variables contain numbers (like height) or categories ("eye color"). More specifically, these differences can be described through the so-called scales of measurement: scales of measurement provide information about which operations are possible with a variable – the higher the scale of measurement is, the more mathematical operations are possible. Higher scales of measurement also allow all operations of the lower ones. Typically, four scales of measurement are distinguished.

#### Scale Level

The scale level of a variable indicates the type of its possible content as well as the arithmetic operations that can be performed with it. There is a hierarchical arrangement (listed from low to high)

1. Nominal scale
    - Pure qualitative scale ("categories")
    - Only frequency determination possible, no further calculations
    - Examples: eye color, favorite animal, gender, country of origin
2. Ordinal scale ("Order")
    - The various characteristics of the variable can be arranged in a meaningful order, without expressing the distance between the ranks in numbers.
    - In addition to the determination of frequency, greater and lesser can also be defined.
    - Examples: Type of school graduation, school grades, placement in car racing, tumor stages I–IV
3. Interval scale
    - Variable takes on a numerical value
    - In addition to frequency determination and classification, a distance (= interval) can also be expressed as a value here.
    - Examples: Intelligence, Year, Temperature in °C
4. Ratio scale (ratio scale, rational scale, proportional scale)
    - Highest scale level
    - The variable also takes on a numeric value here.
    - All basic arithmetic operations are possible, so unlike the interval scale, multiplication and division are also included.
    - Includes a natural zero point: Therefore, 0 must be a possible value of the variable and must also make sense in relation to the described quantity.
    - Examples: body weight or height, blood sugar levels, duration, distance, income

> [!NOTE]
> The order from lowest to highest scale level can be easily remembered using the French word "noir" (= black): nominal, ordinal, interval, and ratio scale.

### Measures of Central Tendency

Measures of Central Tendency are simple parameters that provide information about the measured values of a variable. The most well-known measure of central tendency in everyday life is probably the mean value. Depending on the level of measurement, however, only certain measures of central tendency can be calculated. The measures of central tendency mode, median, and mean are collectively referred to as measures of central tendency.

#### Modal value (= Mode)

As the mode is referred to the most common value of a distribution.

- Definition: Most common value of a distribution
- Can be determined at every scale level
- Advantage: Insensitive to extreme values
- Disadvantages
    - Does not provide information about other characteristic manifestations
    - In multimodal frequency distributions, it is hardly usable.

#### Quantile (Percentiles, Percentiles)

A quantile refers to a value that divides a dataset into two groups: a portion of the dataset is smaller than the defined value, while the remainder is larger. Specific quantiles such as the median and the quartiles are often used. In principle, however, any number between 0 and 100% can be considered a quantile; these general quantiles are then referred to as percentiles or percent ranks.

- Requires at least ordinal-scaled variables (applicable to ordinal, interval, and ratio scales)
- Important quantiles are
    - Median (= ½-Quantile, 50%-Percentile, Percent rank 50)
        - Single value that lies exactly at the middle position of all values after they have been sorted by size.
        - Particularly suitable for non-normally distributed values
        - Above and below him lie 50% of the other values each.
    - Quartile
        - ¼-Quantile (= lower Quartile, 25%-Percentile, Percentile rank 25): 25% of all values are less than this value
        - ¾-Quantile (= upper quartile, 75%-percentile, percentile rank 75): 75% of all values are less than this value
    - Percentiles (= Percent ranks)
        - Can take any values between 0 and 100
        - x%-Percentile means: x% of the values are below this value, all others (that is, 100%-x%) are above it.
        - Beispiele
            - 3%-Percentile (= Percentile rank 3): 3% of all values are less than this value
            - Normal distributions: Here, a percentile of 97.5 is equivalent to the value being two standard deviations above the mean (see also: Gaussian distribution)

#### (Arithmetic) Mean

Averages are specific averages calculated from a distribution. In medical statistics, the arithmetic mean is primarily used.

- Average value that results when all individual values are added together and divided by the number of individual values.
- Requires at least interval-scaled variables (applicable for interval and ratio scales)
- Advantage: Utilizes all information from the distribution
- Disadvantage: Sensitive to extreme values
    - Example
        - 2 + 4 + 5 + 6 + 33 = 50
        - 50 / 5 = 10
        - The mean is 10

> [!NOTE]
> In terms of the scale level, the median is best suited for describing central tendency for an ordinal scale. For the nominal scale, it is the mode, and for the interval and ratio scale, the arithmetic mean is used.

#### Example Measures of Location

We inquire about the age of all 24 participants in a course and receive the following distribution.

|Alter|20|21|22|24|25|50|
|---|---|---|---|---|---|---|
|Frequency|1|8|6|5|3|1|

Now we determine the measures of central tendency introduced above.

- Scale Level: Ratio Scale
    - Meaningful order
    - Distances measurable
    - Conditions determinable
    - Natürlicher Nullpunkt vorhanden
- Mode: 21
    - 21 is the most common age
- Median: 22
    - With 24 participants, the middle is between the 12th and 13th participants, both of whom are 22 years old; thus, 22 divides the field into two equal halves.
        - Odd number of measurements: Median corresponds to the value in the middle
        - Even number of measurements: Median corresponds to the mean of the two values that lie next to the middle
- Quartile: X25 = 21 and X75 = 24
    - 25% are not older than 21 and 75% are not older than 24
- Mean: 23.5
    - (1×20 + 8×21 + 6×22 + 5×24 + 3×25 + 1×50) / 24 ≈ 23.5

One can see that the values of the measure of central tendency differ somewhat. This is because they react differently to outliers. The high mean of about 23.5 is mainly due to one 50-year-old individual. It follows that measure of central tendency alone is not sufficient to characterize a distribution – it is also crucial how closely the values are distributed around the center.

### Streuungsmaße (= Maße der Variabilität)

Often, the measures of central tendency are not sufficient for characterizing a dataset. Various measures of dispersion are used to determine how much the values fluctuate around the mean. The most common measures of dispersion are

- Span: The distance Xmax - Xmin between maximum and minimum is only a very rough measure of the spread and very sensitive to outliers.
- Interquartile Range: The distance between the 75% quartile X75 and the 25% quartile X25 does not take the "edges" into account and is thus less susceptible to outliers.
- Variance: The variance is calculated from the sum of the squared deviations from the mean (M), divided by the number of values (N or n-1)
    - Allgemeine Formel der Varianz:
        - Variance = [(M - X1)² + (M - X2)² + … + (M - XN)²] / N
        - N = total number of the population
    - For samples the following applies:
        - Variance (Sample) = [(M - X1)² + (M - X2)² + … + (M - Xn)²] / (n-1)
        - n = total number of the sample
    - Why is it squared?
        - By squaring, large deviations are weighted more heavily than small ones, and positive and negative deviations do not cancel each other out in this way.
    - Problem of squaring
        - Die Einheit der Variablen wird mitquadriert – dadurch ist die Varianz schwer verwertbar
        - Solution: Calculation of the standard deviation
- Standard Deviation (SD): Taking the square root of the variance yields the standard deviation. It is a measure of how far the individual values are, on average, from the mean.

#### Example Measures of Dispersion

We will again consider the age distribution of the course (see above) and now additionally determine the measure of dispersion.

|Alter|20|21|22|24|25|50|
|---|---|---|---|---|---|---|
|Frequency|1|8|6|5|3|1|

- Wingspan: 30
    - Highest value (50) - lowest value (20) = 30
- Interquartile Range: 3
    - X75(24) - X25(21) = 3
- Variance: ≈ 33
    - ((23.5416666667 - 20)2 + 8×(23.5416666667 - 21)2 + 6×(23.5416666667 - 22)2 + 5×(23.5416666667 - 24)2 + 3×(23.5416666667 - 25)2 + (23.5416666667 - 50)2) / 24 ≈ 33
- Standardabweichung: 5,7
    - √33 ≈ 5,7

The high standard deviation of 5.7 as well as the very large range of 30 indicate that the values fluctuate significantly. The interquartile range of 3 shows that this is likely due to individual outliers (the 50-year-old!); the middle 50% are quite close together.
## Tests and Test Quality Criteria

In psychological and medical diagnostics, tests are used that appear, for example, in the form of a questionnaire. They allow for quantitative statements about constructs, that is, about characteristics that cannot be measured directly. The development of such a test is based on the selection of so-called items (test tasks), which are checked for their quality according to quality criteria.

### Psychological Test

- Serves the quantitative assessment of psychological traits
- Consists of a certain number of items (= test tasks)
    - Criteria for item selection
        - Difficulty Index: Probability of solving an item
        - Discrimination coefficient: The informative value of an item regarding the overall result

### Quality Criteria

#### Objectivity

- Definition: Measure of the independence of test results from the study director, experimenter, or investigator
- Criteria for high objectivity
    - Standardized Test
    - Always the same test results even when evaluated/conducted/interpreted by different individuals.

#### Reliability (Dependability)

- Definition: Measure of the reproducibility of test results under the same conditions
- Kann als Wert zwischen 0 und 1 angegeben werden
    - If the variance of the measurements corresponds solely to the variance of the true values, the value is 1.
    - If the variance of the measurements is entirely due to measurement errors, the value is 0.
    - The closer the reliability is to 1, the more valuable the result.
- Methods for estimating reliability
    - Parallel test reliability: One compares the present test with a similar (parallel) testing procedure.
        - If both test procedures yield similar results, there is a high correlation coefficient and thus a high parallel test reliability
    - Test-Retest Reliability: A test is conducted twice with the same subject and then the results are compared.
        - The retest reliability can be expressed as the correlation coefficient r between the two measurements.
        - Werte von r > 0,8 sprechen für eine gute Reliabilität, diese Anforderung schwankt aber sehr je nach Art des betrachteten Tests
    - Interrater Reliability: When different raters (observers) using the same assessment instrument arrive at the same or similar results, there is a high interrater reliability.
        - This subform of reliability stands for objectivity, as the test results can hardly be influenced by the user of the testing instrument.
    - Internal consistency: Indicates how much the various items (= test tasks) of a study are related to each other.
        - Is determined by the indicator Cronbach's α
        - The higher the internal consistency (or Cronbach's α), the higher the reliability of the study.

#### Validität

- Definition: Measure of the resilience of a specific statement
- Validity of studies: One distinguishes between
    - Internal validity: Are the observed changes in the dependent variable (disease) actually caused by the independent variable (exposure, risk factor) (and not by confounding factors or chance)? To what extent can a causal relationship be derived from the study results?
        - A high internal validity is achieved, among other things, by
            - Study groups with similar characteristics (age, gender, underlying conditions, etc.)
            - Valid and reliable measuring instruments
            - Vermeidung systemischer Fehler (Bias) in der Planung und Durchführung der Studie
    - External Validity: Can the study results be generalized from the small study population to larger segments of the population for which this study is intended? Is this study therefore representative?
        - High external validity is characterized, for example, by:
            - High correlation of the measurements of a new testing procedure with an already established testing procedure
            - Study results can be reproduced in a second separate study with different participants.
            - High internal validity
    - Predictive validity: Can statements about the future be derived based on the test results?
    - Convergent Validity: Do tests for related constructs correlate with each other?
    - Discriminant Validity: Forms the counterpart to convergent validity: Tests for constructs that are not related should also not correlate.
    - Content validity: Does a test capture all relevant aspects of a construct?
    - Change sensitivity: Does a test capture changes in a construct over the course of, for example, an illness or therapy (longitudinal aspect of validity)?

> [!NOTE]
> The hierarchy of quality criteria: The objectivity of a test is a prerequisite for reliability. Reliability is a prerequisite for validity!

#### Standardization

- Definition: Calibration of the test
- Goal: Better classification of the results
- Durchführung: Testung einer repräsentativen Stichprobe

### Fourfold Table

In a test, four results are generally distinguished: The positively tested individuals can be further divided into true positives and false positives, while the negatively tested individuals are similarly divided into true negatives and false negatives. If you arrange these in a table, you obtain the so-called four-field table, which can demonstrate important metrics of tests.

| |Erkrankt|Gesund|Alle Testergebnisse| |
|---|---|---|---|---|
|Test positive|a (true-positive)|b (false-positive)|a + b (all patients with positive test results)|Positive predictive value = a / (a + b)|
|Test negative|c (false-negative)|d (true-negative)|c + d (all patients with negative test results)|Negative predictive value = d / (c + d)|
|All Healthy/Sick|a + c (all sick patients)|b + d (all healthy patients)|a + b + c + d (all patients)||
||True Positive Rate = a / (a + c) = Sensitivity|False Positive Rate = b / (b + d)|||
||False-negative rate = c / (a + c)|True-negative rate = d / (b + d) = Specificity|||

#### Sensitivität (= Empfindlichkeit, Trefferquote)

- = a / (a + c)
- Proportion of those who were correctly tested as "positive" in relation to the total number of patients.
- Indicates how many actually ill individuals are identified as sick by the test.

#### Specificity

- = d / (b + d)
- Proportion of those who were correctly tested as "negative" in relation to the total number of healthy individuals.
- Indicates how many actually healthy individuals were correctly identified as healthy by the test ("True-negative rate")

#### Positive Predictive Value (= Accuracy, Relevance, Positive Predictive Value)

- = a / (a + b)
- Probability of actually being sick with a positive test result
- General calculation: Number of true positive tested individuals divided by the total number of positively tested individuals (i.e., true positive + false positive)
- Depending on the prevalence of a disease in the population

#### Negative Predictive Value (= Discriminatory Power, Negative Predictive Value)

- = d / (c + d)
- Probability of actually being healthy with a negative test result
- General calculation: Number of true negative tested individuals divided by the total number of negatively tested individuals (i.e., true negative + false negative)
- Also dependent on the prevalence of a disease in a population

> [!NOTE]
> To transfer a test result to a patient, the predictive values are important: they additionally take into account how common the tested disease is in general. If you want to explain to a patient what the test result means for them, the predictive values are clearly more suitable than sensitivity and specificity!

#### Example

We are considering an HIV test (immunoassay). This test has a sensitivity of 99.9% and a specificity of 99.8%. The prevalence of HIV in Germany is approximately 100 per 100,000 inhabitants = 0.1%.

- Sensitivity: 99.9%
    - Interpretation: If an HIV-positive subject is tested, the test is 99.9% positive as well.
- Specificity: 99.8%
    - Interpretation: If an HIV-negative subject is tested, the test is 99.8% negative as well.
- Positive predictive value: 33%
- Calculation via PPW = a / (a + b)
    - a = true-positive = Probability that the patient has HIV (= prevalence) × Probability that he is then tested positive (= sensitivity)
    - b = false positive = The probability that the patient does not have HIV (= 1 - prevalence) × The probability that he still tests positive (= 1 - specificity)
    - PPW = (0.999 × 0.001) / (0.999 × 0.001 + 0.002 × 0.999) = 33%
    - Interpretation: If any person tests positive, they are actually 33% HIV positive.
- Negative Predictive Value: 99.999899699%
    - Calculation through NPW = d / (c + d)
    - d = true-negative = probability that the patient does not have HIV (= 1 - prevalence) × probability that he will also test negative (= specificity)
    - c = false negative = The probability that the patient has HIV (= prevalence) × The probability that he is still tested negative (= 1 - sensitivity)
    - NPW = (0.998 × 0.999) / (0.998 × 0.999 + 0.001 × 0.001) = 0.99999899699
    - Interpretation: If any person tests negative, they are 99.999899699% actually HIV-negative.

Here we see how important it is to distinguish between specificity and positive predictive value. A specificity of 99.8% seems very good. However, due to the low prevalence of HIV, it is still more likely (67%!) that a positive result is a false positive.

CAVE: This calculation only applies if any person is tested without indication. If the patient belongs to a risk group, the group-specific prevalence increases, and thus also the positive predictive value!
## Comparison of Risks

### Risk in Epidemiology

In epidemiological studies, the aim is to link an exposure with an event (such as a disease). Therefore, you primarily obtain two groups (exposed and unexposed), which are further divided into diseased and non-diseased. The goal is to demonstrate whether the exposure is associated with an increased risk of disease or not. You can start again with a fourfold table.

|Number of Persons|Exposed|Not Exposed|Total|
|---|---|---|---|
|sick|a|b|a + b|
|healthy|c|d|c + d|
|total|a + c|b + d||

Now various derived quantities can be calculated that provide information about how the risk behaves in the two groups.

#### Absolute Risk (AR)

- Disease risk in a specific population (Incidence)
- Number of new cases per year per 100,000 inhabitants
- Approximately corresponds to (a + b) / (a + b + c + d)

#### Relative Risk (RR)

- Compares the risk of the exposed with that of the unexposed
- Definition: RR = (a / (a + c)) / (b / (b + d))
    - Relative Risk (RR) = Risk in Exposed/Risk in Unexposed
    - Indicates how much greater the risk of illness becomes due to the present risk factor.
- Example
    - 4% of all observed patients without nicotine consumption suffer a heart attack
    - 6% of all observed patients with nicotine consumption suffer a heart attack
    - RR = 6% divided by 4% = 1.5
    - → Smokers have a 1.5 times higher risk of suffering a heart attack in this example.

#### Attributable Risk (= Excess Risk/attributable risk)

- The proportion of the risk that is actually attributable to the risk factor
    - Formula: Risk of the Exposed - Risk of the Non-Exposed = Attributable Risk

#### Odds (R)

- Probability that an event occurs = p
- Chance that an event does not occur = q
    - q = 1 - p
- R = p / q
- Examples
    - Odds of tossing "tails" in a coin flip: Tails / Heads = 1/1 (see absolute risk 0.5)
    - Odds, rolling a 2 on a die: (1/6) / (5/6) = 1/5 (cf. absolute risk 0.167)
- Odds Ratio (OR)
    - Ratio of the chances of two groups for the occurrence of an event
    - Allows an estimation of the relative risk with unknown incidence (especially in case-control studies)
    - Calculation: Quotient of the chance of the affected and the unaffected
        - OR = (a / c) / (b / d) = (a x d) / (b x c)
    - Interpretation
        - OR = 1: Equal opportunity
        - OR > 1: Greater chance for the exposed
        - OR < 1: Chance of the Non-Exposed is greater

### Risk Reduction through Interventions

When an intervention is carried out (e.g., the administration of a blood pressure medication), one wants to know afterwards whether an improvement has been achieved. To do this, one can investigate whether the intervention has led to a reduction in a specific risk (e.g., the risk of dying from a heart attack).

#### Absolute Risk Reduction (ARR)

- Indicates the absolute change in risk due to an intervention.
- ARR = Risk of the control group - Risk of the intervention group
- Example: A change in risk from 3% to 2.5% is an absolute change in risk of (3 - 2.5 =) 0.5 percentage points.

#### Relative Risk Reduction (RRR)

- Indicates the percentage reduction of risk through an intervention.
- RRR = 1 - (Risk of the intervention group / Risk of the control group)
- Example: A change in mortality from 3% to 2.5% is a relative risk reduction of RRR = 1 - (2.5 / 3) = 1 - 0.83 = 0.17 = 17%
- Comparison RRR and ARR
    - Example: A therapy leads to four instead of six deaths in the intervention group of 1,000 people. The relative risk of the intervention group is 0.4% / 0.6% = 0.67 = 67%, while that of the control group is defined as 1 or 100%. The relative risk reduction is therefore RRR = 1 - 0.67 = 0.33 = 33%. The absolute risk reduction is ARR = 0.6% - 0.4% = 0.2%. This is apparently significantly lower (although different parameters are simply being considered here that use the same numbers).

#### Number Needed To Treat (also: NNT)

- Indicates how many patients need to be treated in a given time period to statistically prevent exactly one event.
- A low NNT indicates an effective therapy (large absolute risk reduction)
- Calculation: NNT = Reciprocal of absolute risk reduction (ARR)
- With an NNT = 1, every therapy performed prevents the occurrence of an event.
- Example
    - ARR: A change in risk from 3% to 2.5% is an absolute change in risk of (3 - 2.5 =) 0.5 percentage points (= 0.5 / 100 = 0.005)
    - NNT = Reciprocal of the ARR = 1 / 0.005 = 200 (Therefore, 200 patients need to be treated to prevent one event)
- Number Needed To Screen: Indicates how many patients need to be screened within a certain time frame in order to statistically detect exactly one case of the disease.
- Number Needed To Harm: Indicates how many patients must be exposed to a risk factor within a specific time period for statistically one case of illness to occur.
## Statistical Tests

In a scientific study, data is collected to investigate a hypothesis. The data collection is followed by analysis, which primarily focuses on answering the following questions:

1. Is there a difference between the groups studied?
2. Does a difference rely on the tested hypothesis (“Alternative hypothesis is true”) or only on chance (“Null hypothesis is true”)?
3. How relevant is a difference?

For this, statistical tests are used. Depending on the types of data and how they are distributed, there is a very large number of different tests.

### Statistical Significance

Significance refers to a difference between two results that is so extreme that it can no longer be considered random. It is a criterion for the validity of a result. Whether a result is considered significant depends on the significance level chosen for the respective study: The standard is a significance level of 5%, which means that the probability of a positive result occurring by chance is less than 5%.

#### Significance Level α (syn. α-error level)

- Establishes a probability of error that is still considered acceptable.
- Common: α = 0.05
- Meaning: "A difference is significant at the level of 0.05" → The probability that such a large difference is due to chance is less than 0.05.
    - The lower the significance level, the lower the probability that the null hypothesis will be incorrectly rejected.
    - The lower the significance level, the higher the probability that the alternative hypothesis will be incorrectly rejected.
    - Example: A study is to investigate whether a medication is more effective than the placebo. In fact, the study shows that the medication is more effective. However, the question arises whether this result is merely due to chance (null hypothesis) or whether there is actually a difference in effectiveness (alternative hypothesis). For decision-making, the significance level is set before the start of the study, usually at 0.05 (sometimes also at 0.1 or 0.01). If one were to set the significance level extremely low (e.g., at 0.0005), it is unlikely that the difference would be considered significant (the p-value would thus be below this significance level). In that case, one would incorrectly accept the null hypothesis or reject the alternative hypothesis in many cases and thus assume that the medication and the placebo are equally effective. Since the choice of the significance level can influence the rejection or acceptance of the alternative hypothesis, it is imperative to determine this before the study begins.

> [!NOTE]
> When setting a lower significance level, it becomes more difficult to demonstrate a difference! As a compensation, the group size can be increased.

#### p-value (from English probability)

- Corresponds to the probability that a difference as strong as that in the test result occurs by chance alone (Type I Error)
- Common representation of the calculated significance
- Is calculated from the available data using a suitably chosen statistical test.
- If the measured p-value is below the required significance level, it is said that a difference is significant.
- A low p-value supports the alternative hypothesis, while a high p-value should lead to the retention of the null hypothesis
- Influence of sample size: Large samples are more likely to lead to low p-values and thus to a significant result.

#### Statistical Estimation

When estimating, one wants to derive the unknown true value in the population from collected sample values. It is important here to separate measures that apply to the sample from the estimate, for which one assumes (!) that it could apply in the population.

- Point Estimator
    - Definition: A statistical measure calculated within the sample that serves as an estimate for the population
    - Assumption: Sample is representative of the Population
    - Interpretation: The calculated estimator is approximately identical to the corresponding measure of the population.
- Interval estimator: Extension of the point estimator to include a range within which the true value of the population is likely to lie with high probability.
    - Definition: Interval, which lies around the point estimator
    - Common interval: 95% confidence interval
    - Interpretation of the 95% confidence interval: In 95% of cases, the calculated 95% confidence interval contains the true value from the population, which is estimated using the point estimator
        - Absolute risk reduction or effect size
            - Significant result: 0 is not included in the confidence interval
            - Example: By giving up a recreational substance, the absolute risk of developing a certain disease is reduced by 20%, with a confidence interval of 15%–25%. Since 0% is not within the confidence interval, the absolute risk reduction is significant.
        - Relative Risk, Hazard Ratio or Odds Ratio
            - Significant result: 1 is not included in the confidence interval
            - Example: In the comparison of consumers of a pleasure product to those who abstain from it, the relative risk of developing a certain disease is 1.7, with a confidence interval of 1.6 to 1.8. Since 1 is not within the confidence interval, the difference between the two groups is significant.

> [!NOTE]
> In confidence intervals, you can see the uncertainty of the estimate: The larger a sample is, the smaller the confidence interval becomes, and thus the uncertainty!

#### Test Strength (Power)

- Indicates how well a statistical test is suited to demonstrate a true difference.
- Corresponds to the probability that a test detects a truly existing difference at a given significance level as well.
- Becomes larger through
    - Larger sample
    - Higher significance level
    - The type of statistical test
- Benefit: Needed for planning a study

> [!NOTE]
> Statistically significant means that a difference (with high probability) can not be explained by chance alone. No relevance can be derived from this!

### Effect Size

When a statistically significant difference between two groups has been demonstrated, the question arises as to whether this difference is also relevant. This is of course largely dependent on the hypothesis being studied, but with Cohen's d there is a simple parameter that allows for an estimation.

- Cohen's d
    - Definition: A measure of the size of the effect of an intervention
    - Prerequisite: Normally distributed data
    - Calculation: Difference of the means of the intervention and control group, divided by the standard deviation
        - d = (μ1 - μ2) / σ
        - The effect size decreases as the dispersion of the values increases.
    - Interpretation
        - Relates the difference between two groups to the standard deviation
            - Cohen's d = 1 means that the groups differ by one standard deviation
        - Amount of Cohen's d < 0.2: Small effect
        - Amount of Cohen's d > 0.8: Large effect

> [!NOTE]
> Cohen's d shows how strongly the means of two groups differ. From an amount of 0.8, one speaks of a significant difference!

> [!NOTE]
> Cohen's d can take any values from -∞ to +∞!

> [!NOTE]
> With a small effect size, it is difficult to demonstrate a difference!

### Correlation

It is said that variables correlate when the change in one variable is accompanied by a change in the other variable. Whether variables correlate and how they do so can be described using correlation tests. A simple parameter for this is the correlation coefficient.

#### Correlation Test

- Investigates a relationship between characteristics
    - How does y change when x changes?
    - A complete correlation exists when changes in y can be fully explained by changes in x. No relationship exists when a change in x has no influence on y.

#### Covariance, Correlation Coefficient, and Coefficient of Determination

- Covariance
    - Measure of the linear dependence between two variables
    - Not standardized and therefore only limitedly assessable
- Correlation coefficient
    - Measure of the strength of a correlation
    - Properties: Dimensionless and lies between -1 and +1
    - For at least ordinally scaled data: Rank correlation coefficient ρ according to Spearman
        - "Inaccurate", not all information can be used
        - Extremes have less weight.
        - Does not assume a normal distribution
    - For interval-scaled data: Correlation coefficient r according to Pearson
        - More accurate, but more susceptible to distortions caused by outliers
        - Corresponds to the standardized covariance
        - Assumes approximately normally distributed data
- Coefficient of determination R2
    - Measure of the proportion of variance of one variable that is explained by the other variable
    - Coefficient of determination = Correlation coefficient^2

> [!NOTE]
> The covariance allows a statement about the direction of a correlation, while the correlation coefficient also indicates the strength!

##### Positive Correlation

- The higher y, the higher x is as well.
- Values between 0 and 1 indicate a positive correlation
- In a maximum positive correlation, the correlation coefficient has a value of +1.

##### Negative Correlation

- The higher y, the lower x is.
- Values between 0 and -1 indicate a negative correlation
- In the case of a maximum negative correlation, the correlation coefficient has a value of -1.

> [!NOTE]
> The correlation coefficient describes how clear the relationship between two variables is; from a value of about +/-0.7, one speaks of a strong positive/negative relationship!

## Review Questions for the Chapter Medical Statistics and Test Theory
What is a latent construct?
- A latent construct is a concept that cannot be measured directly, such as quality of life or health.
What does the term "operationalization" refer to?
- A latent construct like intelligence cannot be quantified directly; therefore, during the planning of a study, a procedure for measurement and suitable measurement instruments must be developed. This is called operationalization.
What different types of variables do you know that can be observed within studies or have an impact on the outcome?
- In a study, the independent (= explanatory) variable is deliberately changed to measure changes in the dependent (= explained) variable. There are also third variables that can influence the study outcome: The confounding variable (= confounder) can affect both the dependent and independent variable but was not considered in the study. The mediator variable mediates the effect of the independent variable on the dependent variable. The moderator variable, on the other hand, influences the independent and thereby the dependent variable. The risk indicator points to a disease but, unlike the risk factor, has no causal relationship with it.
What levels of measurement are there and what do they mean? How can they be arranged hierarchically?
- There are four scale levels, which arise from the type of their variable and the mathematical operations that can be performed. Based on these operations, they can be arranged hierarchically from low to high as follows: The nominal scale qualitatively names categories; only frequency determinations are possible. On the ordinal scale, the attributes of a variable can be put in order. Terms like "greater" and "smaller" can be applied. The interval scale additionally allows determining the distance as the difference between attributes. The highest rank is the ratio scale, where all arithmetic operations are possible and which has a natural zero point. Higher scales always allow all the operations of the lower scale levels.
Assign the appropriate scale levels to the following examples: Mood (good, medium, bad) - Eye color (blue, green, brown) - Money in the account (in Euros) - Temperature in °C (exact degree number).
- The classification "good, medium, bad" for mood allows an ordering of variables. The scale level thus corresponds to an ordinal scale. The terms "blue, green, brown" are purely qualitative categories, so it is a nominal scale. When measuring the account balance, there is a zero point (bankruptcy), and comparisons can be made (X has twice as much money as Y). This is an example of a ratio scale. The temperature in °C, however, has no natural zero point but one arbitrarily set at the freezing point of water. It can nevertheless be said that the refrigerator at 8 °C is 12 °C colder than room temperature at 20 °C. Therefore, it is an interval scale.
Name the measures of central tendency. Explain how they are determined and for which scales they are suitable!
- The measures of central tendency are mode, median, and arithmetic mean. The mode indicates the most frequent value of a distribution and can be determined for any scale level. The median is the value exactly in the middle of the sorted values. It is applicable for ordinal, interval, and ratio scales. The arithmetic mean is calculated by adding all individual values and dividing by the number of values. It is applicable for interval and ratio scales.
What are variance and standard deviation? How are they calculated?
- Variance and standard deviation are measures of deviation of measurements from the mean. Variance is calculated as the sum of squared deviations from the mean (M) divided by the number of values (n). The standard deviation corresponds to the square root of the variance.
What do range and interquartile range indicate, and how are they influenced by extreme values (so-called outliers)?
- Range and interquartile range are measures of dispersion. The range indicates the difference between the highest and lowest value and is very sensitive to outliers. The interquartile range denotes the distance between the 75% quartile and the 25% quartile. Since the "edges" are not considered here, this measure is less sensitive to outliers.
What is a percentile and what specific percentiles are there?
- The percentile can take any percentage value between 0 and 100. An x%-percentile divides a set into two groups, with x% of values lying below this percentile and the rest (100%-x%) above. The median is the 50th percentile, i.e., 50% of values lie above and below. The 1st quartile or lower quartile is the 25th percentile, i.e., 25% of values lie below and 75% above. The 3rd quartile or upper quartile is the 75th percentile, i.e., 75% of values lie below and 25% above.
What percentile corresponds to a value in a normal distribution that is two standard deviations above the mean?
- In a normal distribution, 5% of measurements lie more than two standard deviations away from the mean, with 2.5% below and 2.5% above. Therefore, values within two standard deviations above the mean correspond to: 100% - 2.5% = 97.5%. The percentile rank of the value is thus 97.5%.
### Tests and Test Quality Criteria

What test quality criteria are there and how can they be organized hierarchically?
- There are three quality criteria for tests: objectivity, reliability, and validity. Objectivity measures the independence of results from the examiner. It is a prerequisite for reliability, i.e., the reproducibility of test results under the same conditions. Reliability is in turn a prerequisite for validity, a measure of the robustness of a statement.
What different methods for estimating reliability do you know, and how is each of these conducted?
- To assess parallel test reliability, the test is compared with a similar test procedure. For test-retest reliability, the same test is administered twice to the same participant. When multiple raters conduct the test and come to very similar results, there is high interrater reliability. In this case, the test results hardly depend on the user of the test instrument, so interrater reliability also stands for objectivity. Internal consistency describes how strongly the different items (= test tasks) within one test relate to each other, i.e., how strongly the different parts of a test yield consistent results.
How can the test-retest reliability be quantified and what is considered a high value? Please refer to the examples "intelligence test" and "scale".
- Test-retest reliability can be quantified using the correlation coefficient. A high correlation coefficient r between two measurements indicates good test-retest reliability. Generally, r values >0.8 are considered good (e.g., in intelligence tests). However, this requirement varies greatly depending on the test: for example, for a scale, r should be nearly 1.
What is validity and what forms of validity are distinguished?
- Validity is a measure of the robustness of a statement. Internal validity describes the extent to which a causal relationship can be derived from the study results. External validity assesses whether results from the study group can be generalized to a larger population. When results from the study can be used to predict the future, it is called predictive validity. Convergent validity checks whether different tests for content-related constructs correlate. The counterpart is discriminant validity, i.e., tests for unrelated constructs should not correlate.
What is normalization and how can it be carried out?
- Normalization is the calibration of the test to better classify the results. Based on a representative sample, what is "normal" is determined. If the results of individual subjects differ, for example, by more than two standard deviations from the sample mean, the results are usually considered unusual.
What are sensitivity, specificity, positive and negative predictive value and how are they calculated? If necessary, draw a four-field table.
- Sensitivity is the "true positive rate" of a test. It indicates the proportion of tested persons who were correctly tested "positive" relative to the total number of sick individuals. Specificity is the "true negative rate" of a test, calculated as the quotient of those correctly tested "negative" and the total number of healthy individuals. The positive predictive value indicates the probability of actually being sick when the test result is positive. The negative predictive value indicates the probability of truly being healthy when the test result is negative.
How does the prevalence of a disease affect the following parameters of a test for diagnosing this condition: sensitivity, specificity, positive and negative predictive value?
- Sensitivity and specificity are specific, constant properties of a test and are therefore not influenced by external factors such as disease prevalence. The positive predictive value, however, indicates the probability of actually being sick if the test result is positive. With low prevalence, the healthy group is very large, so relatively more false positive values (b) can arise compared to true positive values (a). Thus, the positive predictive value PPV = a / (a + b) decreases with decreasing prevalence. Conversely, the negative predictive value NPV = d / (c + d) rises with low prevalence, since the proportion of truly sick and thus also the number of false negative test results (c) decreases while the number of true negative tests (d) increases. Simple mnemonic: The POSITIVE predictive value correlates POSITIVELY with prevalence – if one decreases, the other decreases too. The NEGATIVE predictive value correlates NEGATIVELY with prevalence – if one decreases, the other increases. Sensitivity and specificity have neither positive nor negative in their names, so they do not change with prevalence.
What does the change sensitivity indicate?
- Change sensitivity indicates how sensitive a test is to changes in the measured variable.
### Comparison of Risks

Explain the terms “absolute risk”, “relative risk”, and “attributable risk”!
- Absolute risk indicates the disease risk in a specific population. Relative risk includes exposure to a risk factor and compares the risk of the exposed with the risk of the non-exposed. Attributable risk is calculated as the difference between the risk of the exposed and the risk of the non-exposed. It indicates the proportion of the risk actually attributable to the risk factor.
What do the terms “Odds” and “Odds Ratio” mean? How are they calculated?
- Odds express the chance that an event occurs. It is the quotient of the probability that an event occurs divided by the probability that the event does not occur (e.g., 1/1 in a coin toss). The odds ratio includes exposure to an event and is calculated as the quotient of the odds of the exposed and the odds of the non-exposed. It enables, especially in case-control studies, an estimate of relative risk when incidence is unknown. An odds ratio = 1 indicates equal chances; >1 means the chance of the exposed is higher; <1 means the chance of the non-exposed is higher to become ill.
How are absolute and relative risk reductions calculated, and how can they be interpreted?
- Absolute risk reduction is calculated from the difference between the risk of the control group and the risk of the intervention group. It indicates the absolute change of risk due to an intervention. In contrast, the relative risk reduction indicates the percentage decrease of risk due to an intervention. It is calculated as 1 minus the quotient of the risk of the intervention group and the risk of the control group.
What does the Number Needed To Treat indicate and how is it calculated?
- The Number Needed To Treat (NNT) indicates how many patients must be treated to prevent exactly one event. It corresponds to the reciprocal of the absolute risk reduction.
### Statistical Tests

Based on which metric can one assess whether a hypothesis is accurate?
- The p-value gives the probability of a Type I error, i.e., whether the study result occurred by chance. To evaluate the p-value, a significance level is set, usually at α = 0.05. If the p-value is below this, a difference is considered significant. This means the probability that the study result occurred by chance is below 5%.
Explain the term “test strength” and name factors that increase the test strength.
- Test strength or power indicates how well a statistical test is suited to detect an actual difference. A high test strength is achieved through a larger sample, a higher significance level, and the type of statistical test.
How can it be mathematically estimated whether a research result is relevant? Name a measure for this and its threshold!
- Effect size is used to assess the relevance of a result. One measure is Cohen’s d, which compares the means of the intervention and control groups. It is calculated as the difference of means between intervention and control groups divided by the standard deviation. A value >0.8 indicates a substantial difference.
How does a greater dispersion affect the effect size?
- Effect size is calculated as the difference between means of two groups divided by the standard deviation. If the standard deviation (a measure of dispersion) increases, the effect size decreases.
Explain in general what correlation means. How do the variables "speed of a car" and "distance covered within 1 hour" as well as "driving distance of a car" and "gasoline in the tank" correlate with each other? Also, name a measure for the strength of the relationship. Explain what values it can take and what is considered a high value!
- The faster a car drives, the longer the distance covered within one hour. These two variables correlate positively. However, with increasing driving distance, the gasoline in the tank decreases – these variables correlate negatively. Generally, "correlation" means that the change in one variable is associated with a change in another variable. The measure for the strength of correlation is the correlation coefficient, which can be determined by different methods. Positive correlation is indicated by values between 0 and 1; the higher one variable is, the higher the other. Values between 0 and -1 indicate a negative correlation; the higher one variable is, the lower the other. Values about +/-0.7 are regarded as strong positive/negative relationships.